# Evaluating data

Here we lay out the steps to evaluate the data generated by a test run.
Pre-requisite is, that you have run the docker enviornment.
See how to do that by reading the `Readme.md`s in [`../`](https://github.com/informagi/ipfsearch), [`../Data`](https://github.com/informagi/ipfsearch/tree/master/Data) and [`../DockerSetup`](https://github.com/informagi/ipfsearch/tree/master/DockerSetup).

## `Eval.py`
See `python Eval.py -h` for information on how to use the script.
For now, you can use it without arguments or flags.

It generates a markdown file (default: `output.md`) with all information on the run presented in simple tables.
(TODO:)
Furthermore, it generates search result files in the correct format to be analysed by [`trec_eval`](https://github.com/usnistgov/trec_eval).
If your dataset provides a ground-truth for relevance of search results: great!
If not, we recommend comparing performance of different runs and treating one of them as truth.
For example, run the setup with one node and no topic sharding.
This is the (assumed) ground truth.
If you now run another setup with multiple nodes and topic sharding enabled,
you will see a drop in quality of search results.
BUT: What did you gain in performance?
The indices are smaller and instead of always querying one node,
the workload is distributed among the peers.
